{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data refactoring\n",
    "\n",
    "The aim of this script is to recompile the entire data in a single CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = { \n",
    "    \"No_noise_static\" : \"my_results/194202_mia_nonoise_static_niidattacks/\",\n",
    "    \"No_noise_dynamic\" : \"my_results/1942107_mia_nonoise_dynamic_niidattacks/\",\n",
    "\n",
    "    \"Gaussian64_static\": \"my_results/1943115_static_gaussiannoise64th_niidattacks/\",\n",
    "    \"Gaussian64_dynamic\" : \"my_results/1943116_dynamic_gaussiannoise64th_niidattacks/\",\n",
    "    \"Gaussian32_static\": \"my_results/1942103_mia_gaussian32th_static_niidattacks/\",\n",
    "    \"Gaussian32_dynamic\" : \"my_results/1942104_mia_gaussian32th_dynamic_niidattacks/\",\n",
    "    \"Gaussian16_static\" : \"my_results/1943162_static_gaussiannoise16th_niidattacks/\",\n",
    "    \"Gaussian16_dynamic\" : \"my_results/1943163_dynamic_gaussiannoise16th_niidattacks/\",\n",
    "    \"Gaussian8_static\": \"my_results/1942108_mia_gaussian8th_static_niidattacks/\",\n",
    "    \"Gaussian8_dynamic\": \"my_results/1942109_mia_gaussian8th_dynamic_niidattacks/\",\n",
    "    \"Gaussian4_static\": \"my_results/1942701_static_gaussiannoise4th_niidattacks/\",\n",
    "    \"Gaussian4_dynamic\" : \"my_results/1942702_dynamic_gaussiannoise4th_niidattacks/\",\n",
    "    \"Gaussian2_static\": \"my_results/1942845_static_gaussiannoise2th_niidattacks/\",\n",
    "    \"Gaussian2_dynamic\" : \"my_results/1942843_dynamic_gaussiannoise2th_niidattacks/\",\n",
    "\n",
    "    \"ZeroSum64_static\": \"my_results/1943117_static_zerosum64th_niidattacks/\" ,\n",
    "    \"ZeroSum64_dynamic\" : \"my_results/1943118_dynamic_zerosum64th_niidattacks/\",\n",
    "    \"ZeroSum32_static\":\"my_results/1942105_mia_zerosum32th_static_niidattacks/\" ,\n",
    "    \"ZeroSum32_dynamic\" : \"my_results/1942106_mia_zerosum32th_dynamic_niidattacks/\",\n",
    "    \"ZeroSum16_static\": \"my_results/1943164_static_zerosum16th_niidattacks/\",\n",
    "    \"ZeroSum16_dynamic\" : \"my_results/1943165_dynamic_zerosum16th_niidattacks/\",\n",
    "    \"ZeroSum8_static\": \"my_results/1942112_mia_zerosum8th_static_niidattacks/\",\n",
    "    \"ZeroSum8_dynamic\" : \"my_results/1942113_mia_zerosum8th_dynamic_niidattacks/\",\n",
    "    \"ZeroSum4_static\": \"my_results/1942703_static_zerosum4th_niidattacks/\",\n",
    "    \"ZeroSum4_dynamic\" : \"my_results/1942704_dynamic_zerosum4th_niidattacks/\",\n",
    "    \"ZeroSum2_static\": \"my_results/1942844_static_zerosum2th_niidattacks/\",\n",
    "    \"ZeroSum2_dynamic\" : \"my_results/1942846_dynamic_zerosum2th_niidattacks/\",\n",
    "\n",
    "\n",
    "    \"Muffliato64_static\": \"my_results/1954498_static_muffliato_niidattacks_64th/\",\n",
    "    \"Muffliato64_dynamic\" : \"my_results/1954499_dynamic_muffliato_niidattacks_64th/\",\n",
    "    \"Muffliato32_static\": \"my_results/1954496_static_muffliato_niidattacks_32th/\",\n",
    "    \"Muffliato32_dynamic\" : \"my_results/1954497_dynamic_muffliato_niidattacks_32th/\",\n",
    "    \"Muffliato16_static\" : \"my_results/1954494_static_muffliato_niidattacks_16th/\",\n",
    "    \"Muffliato16_dynamic\" : \"my_results/1954495_dynamic_muffliato_niidattacks_16th/\",\n",
    "    \"Muffliato8_static\": \"my_results/1954501_static_muffliato_niidattacks_8th/\",\n",
    "    \"Muffliato8_dynamic\": \"my_results/1954500_dynamic_muffliato_niidattacks_8th/\",\n",
    "    \"Muffliato4_static\": \"my_results/1954503_static_muffliato_niidattacks_4th/\",\n",
    "    \"Muffliato4_dynamic\" : \"my_results/1954502_dynamic_muffliato_niidattacks_4th/\",\n",
    "    \"Muffliato2_static\": \"my_results/1954505_static_muffliato_niidattacks_2th/\",\n",
    "    \"Muffliato2_dynamic\" : \"my_results/1954504_dynamic_muffliato_niidattacks_2th/\",\n",
    "}\n",
    "\n",
    "folder_name = \"formatted_results/36nodes\"\n",
    "\n",
    "\n",
    "TOTAL_PROCESSES = 36\n",
    "MAX_MACHINES =  3\n",
    "STARTING_ITERATION = 0\n",
    "MAX_ITERATIONS=4000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import nansum\n",
    "from numpy import nanmean\n",
    "\n",
    "import os\n",
    "import threading\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "assert TOTAL_PROCESSES%MAX_MACHINES == 0\n",
    "MAX_PROCESSES = TOTAL_PROCESSES//MAX_MACHINES\n",
    "\n",
    "machine_folder = 'machine{}'\n",
    "result_file = '{}_results.json'\n",
    "\n",
    "\n",
    "\n",
    "def load_data(dir):\n",
    "    data = pd.DataFrame({})\n",
    "    for machine in range(MAX_MACHINES):\n",
    "        for rank in range(MAX_PROCESSES):\n",
    "            print(f\"Loading results for machine {machine} and rank {rank}.  \",end = \"\\r\")\n",
    "            uid = rank + machine * MAX_PROCESSES\n",
    "\n",
    "            file = os.path.join(dir, machine_folder.format(machine), result_file.format(rank))\n",
    "            tmp_df = pd.read_json(file)\n",
    "            tmp_df[\"uid\"] = uid # Manually add the uid for further processing                                                                   \n",
    "            tmp_df[\"iteration\"] = tmp_df.index\n",
    "            # print(tmp_df)\n",
    "            tmp_df = tmp_df[tmp_df[\"iteration\"]>=STARTING_ITERATION]\n",
    "            tmp_df = tmp_df[tmp_df[\"iteration\"]<=MAX_ITERATIONS]\n",
    "            data = pd.concat([data,tmp_df])\n",
    "    return data\n",
    "    \n",
    "# Load the data\n",
    "# data_dict = {}\n",
    "# for (key, dir) in target_dir.items():\n",
    "#     print(f\"Loading results for run {key} at folder {dir}.  \")\n",
    "#     data_dict[key] = load_data(dir)\n",
    "# print(\"Loading finished!\" + \" \"*40)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load privacy data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCATIONS_OF_ATTACKS = [\"PRE-STEP\",\"PRE-STEP-niid\"]\n",
    "\n",
    "\n",
    "\n",
    "assert TOTAL_PROCESSES%MAX_MACHINES == 0\n",
    "MAX_PROCESSES = TOTAL_PROCESSES//MAX_MACHINES\n",
    "\n",
    "machine_folder = 'machine{}'\n",
    "privacy_folder = 'privacy'\n",
    "summary_folder = 'summary'\n",
    "process_folder = '{}'\n",
    "\n",
    "def load_privacy_data(path_dir):\n",
    "    data = {}\n",
    "\n",
    "    for loc in LOCATIONS_OF_ATTACKS:\n",
    "        location = f\"privacy-summary-{loc}.json\"\n",
    "        data[loc] = pd.DataFrame({})  \n",
    "        for machine in range(MAX_MACHINES):\n",
    "            for rank in range(MAX_PROCESSES):\n",
    "                print(f\"Loading {location} for machine {machine} and rank {rank}  \",end = \"\\r\")\n",
    "                file = os.path.join(path_dir, machine_folder.format(machine),privacy_folder, summary_folder, process_folder.format(machine*MAX_PROCESSES+rank), location)\n",
    "                tmp_df = pd.read_json(file)\n",
    "                tmp_df = tmp_df[tmp_df.iteration <= MAX_ITERATIONS]\n",
    "                tmp_df = tmp_df[tmp_df.iteration >= STARTING_ITERATION]\n",
    "                #tmp_df['location_of_attack']= file.split('.')[0]\n",
    "                data[loc] = pd.concat([data[loc],tmp_df])\n",
    "    return data\n",
    "\n",
    "# privacy_data_dict = {}\n",
    "# for key,dir in target_dir.items():\n",
    "#     print(f\"Loading privacy data for {key} at \\\"{dir}\\\"\")\n",
    "#     privacy_data_dict[key] = load_privacy_data(dir)\n",
    "# print(\"Loading finished!\" + \" \"*40)\n",
    "\n",
    "# data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['iteration','test_acc', 'test_niid_acc']\n",
    "agg_methods = [\"mean\", \"min\", \"max\"]\n",
    "privacy_columns = ['iteration', 'Attacker advantage']\n",
    "\n",
    "\n",
    "def format_data(key,dir):\n",
    "    filename = f\"{key}.csv\"\n",
    "    output_path = f\"{folder_name}/{filename}\"\n",
    "    if filename in os.listdir(folder_name):\n",
    "        print(f\"Data for {key} already formatted!\")\n",
    "        return\n",
    "    print(f\"Started formatting data for {key}\" + \" \" *40)\n",
    "    gen_data = load_data(dir)\n",
    "    usable_data = gen_data[columns].dropna()\n",
    "    \n",
    "    usable_data = usable_data.groupby('iteration').agg(agg_methods)\n",
    "    usable_data.reset_index(inplace=True)\n",
    "\n",
    "    usable_data.insert(1,\"experience_name\",key)\n",
    "\n",
    "    usable_data.columns = [' '.join(e) if len(e[-1])>0 else e[0] for e in usable_data.columns]\n",
    "\n",
    "    usable_data.set_index(\"iteration\",inplace=True)\n",
    "\n",
    "\n",
    "    privacy_data = load_privacy_data(dir)\n",
    "    for loc in LOCATIONS_OF_ATTACKS:\n",
    "        privacy_data_loc = privacy_data[loc]\n",
    "\n",
    "        privacy_data_loc = privacy_data_loc[privacy_data_loc[\"slice feature\"] == \"Entire dataset\"]\n",
    "        averaged = privacy_data_loc[privacy_columns].groupby('iteration').agg([\"mean\"])\n",
    "        averaged.columns = list(map(' '.join, averaged.columns.values))\n",
    "        averaged.reset_index(inplace=True)\n",
    "        \n",
    "        # averaged.drop(1,axis=1)\n",
    "        # print(averaged[\"Attacker advantage\"])\n",
    "        averaged.rename(columns = {('Attacker advantage mean'): f\"Attacker advantage mean {loc}\"}, errors=\"raise\", inplace=True)\n",
    "        # print(usable_data)\n",
    "        # print(privacy_data_loc)\n",
    "        # privacy_data \n",
    "        averaged.set_index(\"iteration\",inplace=True)\n",
    "\n",
    "        # print(usable_data)\n",
    "        # print(averaged)\n",
    "        usable_data = usable_data.join(averaged, on = 'iteration')\n",
    "        # print(\"Joined:\")\n",
    "        # print(usable_data)\n",
    "\n",
    "    # print(usable_data)\n",
    "\n",
    "    # Save the formatted result \n",
    "    \n",
    "    usable_data.to_csv(output_path)\n",
    "    print(f\"Formatted data for {key} at {output_path}\" + \" \" *40)\n",
    "    return\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    for key,dir in target_dir.items():\n",
    "        executor.submit(format_data,key,dir)\n",
    "        # format_data(key,dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
